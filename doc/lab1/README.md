#### Lab 1: MapReduce

##### 一、实验概览

![MapReduce](https://github.com/zjsyhjh/distributed-system/blob/master/doc/lab1/lab1-1.png?raw=true)

- 本实验中，MapReduce支持两种模式，分别为Sequential和Distributed
  - Sequential模式下，map任务按序一个个执行，执行完所有map任务之后，开始执行reduce任务，同样也是按序执行
  - Distributed模式下， Master将map任务分发给空闲的Worker，待所有map任务执行完之后，reduce任务开始执行




##### 二、源码说明

- MapReduce的实现位于mapreduce包下，测试程序通过调用master.go中的`Sequential()`或`Distributed()`来启动job

  - `Sequential`模式下，就只有Master自己工作，启动Master之后，将输入的文件切分成指定的份数，提交给Map任务，需要的参数包括发起这个job的名称，需要输入的文件，一个map函数和reduce函数，以及指定的reduce任务的数量	

    - ```go
      func TestSequentialSingle(t *testing.T) {
      	mr := Sequential("test", makeInputs(1), 1, MapFunc, ReduceFunc)
      	mr.Wait()
      	check(t, mr.files)
      	checkWorker(t, mr.stats)
      	cleanup(mr)
      }
      ```

    ​

  - 通过查看`Sequential()`函数可以看到，该函数的功能就是有序的一个个地运行map以及reduce任务，Map阶段的任务交给`doMap`函数，Reduce阶段的任务交个`doReduce`函数

    - ```go
      // Sequential runs map and reduce tasks sequentially, waiting for each task to
      // complete before running the next.
      func Sequential(jobName string, files []string, nreduce int,
      	mapF func(string, string) []KeyValue,
      	reduceF func(string, []string) string,
      ) (mr *Master) {
      	mr = newMaster("master")
      	go mr.run(jobName, files, nreduce, func(phase jobPhase) {
      		switch phase {
      		case mapPhase:
      			for i, f := range mr.files {
      				doMap(mr.jobName, i, f, mr.nReduce, mapF)
      			}
      		case reducePhase:
      			for i := 0; i < mr.nReduce; i++ {
      				doReduce(mr.jobName, i, mergeName(mr.jobName, i), len(mr.files), reduceF)
      			}
      		}
      	}, func() {
      		mr.stats = []int{len(files) + nreduce}
      	})
      	return
      }

      //mr.run
      func (mr *Master) run(jobName string, files []string, nreduce int,
      	schedule func(phase jobPhase),
      	finish func(),
      ) {
      	mr.jobName = jobName
      	mr.files = files
      	mr.nReduce = nreduce

      	fmt.Printf("%s: Starting Map/Reduce task %s\n", mr.address, mr.jobName)

      	schedule(mapPhase)
      	schedule(reducePhase)
      	finish()
      	mr.merge()

      	fmt.Printf("%s: Map/Reduce task completed\n", mr.address)

      	mr.doneChannel <- true
      }
      ```

  - `doMap`以及`doReduce`的代码就是`part-I`要完成的内容，`doMap`通过读取文件内容，给每个词进行计数，然后将KeyValue结果根据`Key的哈希值 % nReduce`的结果放到响应的nReduce个中间文件，当map任务完成时，就会产生`nMap * nReduce`个中间文件，其中每个文件的格式为`mrtmp.xxx-mapid-reduceid`，每个文件名由`reduceName`函数生成，Key哈希值相同的KeyValue放在同一个文件中

    - ```go
      // doMap manages one map task: it reads one of the input files
      // (inFile), calls the user-defined map function (mapF) for that file's
      // contents, and partitions the output into nReduce intermediate files.
      func doMap(
      	jobName string, // the name of the MapReduce job
      	mapTaskNumber int, // which map task this is
      	inFile string,
      	nReduce int, // the number of reduce task that will be run ("R" in the paper)
      	mapF func(file string, contents string) []KeyValue,
      ) {
      	//
      	// You will need to write this function.
      	//
      	// The intermediate output of a map task is stored as multiple
      	// files, one per destination reduce task. The file name includes
      	// both the map task number and the reduce task number. Use the
      	// filename generated by reduceName(jobName, mapTaskNumber, r) as
      	// the intermediate file for reduce task r. Call ihash() (see below)
      	// on each key, mod nReduce, to pick r for a key/value pair.
      	//
      	// mapF() is the map function provided by the application. The first
      	// argument should be the input file name, though the map function
      	// typically ignores it. The second argument should be the entire
      	// input file contents. mapF() returns a slice containing the
      	// key/value pairs for reduce; see common.go for the definition of
      	// KeyValue.
      	//
      	// Look at Go's ioutil and os packages for functions to read
      	// and write files.
      	//
      	// Coming up with a scheme for how to format the key/value pairs on
      	// disk can be tricky, especially when taking into account that both
      	// keys and values could contain newlines, quotes, and any other
      	// character you can think of.
      	//
      	// One format often used for serializing data to a byte stream that the
      	// other end can correctly reconstruct is JSON. You are not required to
      	// use JSON, but as the output of the reduce tasks *must* be JSON,
      	// familiarizing yourself with it here may prove useful. You can write
      	// out a data structure as a JSON string to a file using the commented
      	// code below. The corresponding decoding functions can be found in
      	// common_reduce.go.
      	//
      	//   enc := json.NewEncoder(file)
      	//   for _, kv := ... {
      	//     err := enc.Encode(&kv)
      	//
      	// Remember to close the file after you have written all the values!
      	//

      	bytes, err := ioutil.ReadFile(inFile)
      	if err != nil {
      		log.Fatal("doMap: ", err)
      	}
      	//get []KeyValue
      	res := mapF(inFile, string(bytes))
      	//create nReduce Files
      	var files []*os.File
      	for i := 0; i < nReduce; i++ {
      		f, err := os.Create(reduceName(jobName, mapTaskNumber, i))
      		files = append(files, f)
      		if err != nil {
      			log.Fatal("doMap: Create ", err)
      		}
      	}
      	for _, f := range files {
      		defer f.Close()
      	}

      	//put kv into right file
      	for _, kv := range res {
      		index := ihash(kv.Key) % nReduce
      		enc := json.NewEncoder(files[index])
      		err := enc.Encode(&kv)
      		if err != nil {
      			log.Fatal("doMap: Encode ", err)
      		}
      	}
      }
      ```

  - map任务完成之后，Master接着调用`doReduce`函数，reduce任务将后缀reduceid相同的文件合并为一个文件（合并相同`ihash(key)`的文件），这样就一共生成nReduce个文件，每个文件名由`mergeName`函数产生

    - ```go
      // doReduce manages one reduce task: it reads the intermediate
      // key/value pairs (produced by the map phase) for this task, sorts the
      // intermediate key/value pairs by key, calls the user-defined reduce function
      // (reduceF) for each key, and writes the output to disk.
      func doReduce(
      	jobName string, // the name of the whole MapReduce job
      	reduceTaskNumber int, // which reduce task this is
      	outFile string, // write the output here
      	nMap int, // the number of map tasks that were run ("M" in the paper)
      	reduceF func(key string, values []string) string,
      ) {
      	//
      	// You will need to write this function.
      	//
      	// You'll need to read one intermediate file from each map task;
      	// reduceName(jobName, m, reduceTaskNumber) yields the file
      	// name from map task m.
      	//
      	// Your doMap() encoded the key/value pairs in the intermediate
      	// files, so you will need to decode them. If you used JSON, you can
      	// read and decode by creating a decoder and repeatedly calling
      	// .Decode(&kv) on it until it returns an error.
      	//
      	// You may find the first example in the golang sort package
      	// documentation useful.
      	//
      	// reduceF() is the application's reduce function. You should
      	// call it once per distinct key, with a slice of all the values
      	// for that key. reduceF() returns the reduced value for that key.
      	//
      	// You should write the reduce output as JSON encoded KeyValue
      	// objects to the file named outFile. We require you to use JSON
      	// because that is what the merger than combines the output
      	// from all the reduce tasks expects. There is nothing special about
      	// JSON -- it is just the marshalling format we chose to use. Your
      	// output code will look something like this:
      	//
      	// enc := json.NewEncoder(file)
      	// for key := ... {
      	// 	enc.Encode(KeyValue{key, reduceF(...)})
      	// }
      	// file.Close()
      	//
      	kvs := make(map[string][]string)
      	for i := 0; i < nMap; i++ {
      		//get the KeyValue of mrtmp-jobName-i-reduceTaskNumber, then put them into kvs
      		//example : put the key of mrtmp.test-0-0, mrtmp.test-1-0, mrtmp.test-2-0 into outputfile(mrtmp-test-0)
      		fileName := reduceName(jobName, i, reduceTaskNumber)
      		file, err := os.Open(fileName)
      		defer file.Close()
      		if err != nil {
      			log.Fatal("doReduce: Open ", err)
      		}

      		dec := json.NewDecoder(file)
      		for {
      			var kv KeyValue
      			err = dec.Decode(&kv)
      			if err != nil {
      				break
      			}
      			// key -> {value1, value2, value3, ... }
      			kvs[kv.Key] = append(kvs[kv.Key], kv.Value)
      		}
      	}

      	var keys []string
      	for k := range kvs {
      		keys = append(keys, k)
      	}
      	sort.Strings(keys)

      	file, err := os.Create(outFile)
      	defer file.Close()
      	if err != nil {
      		log.Fatal("doReduce: Create ", err)
      	}
      	enc := json.NewEncoder(file)
      	for _, k := range keys {
      		//reduceF: (key, {value1, value2, ...}) -> (key, value)
      		enc.Encode(KeyValue{k, reduceF(k, kvs[k])})
      	}
      }
      ```

  - 最后通过`merge`函数将nReduce个文件结果进行合并，Master发送一个`true`到`doneChannel`来表明任务完成

  -   `Distributed`模式下，Master启动时通过调用`startRPCServer`将自己注册到RPC服务中，并且监听`Unix Socket`等待Worker的连接

      -   ```go
          // startRPCServer starts the Master's RPC server. It continues accepting RPC
          // calls (Register in particular) for as long as the worker is alive.
          func (mr *Master) startRPCServer() {
          	rpcs := rpc.NewServer()
          	rpcs.Register(mr)
          	os.Remove(mr.address) // only needed for "unix"
          	l, e := net.Listen("unix", mr.address)
          	if e != nil {
          		log.Fatal("RegstrationServer", mr.address, " error: ", e)
          	}
          	mr.l = l

          	// now that we are listening on the master address, can fork off
          	// accepting connections to another thread.
          	go func() {
          	loop:
          		for {
          			select {
          			case <-mr.shutdown:
          				break loop
          			default:
          			}
          			conn, err := mr.l.Accept()
          			if err == nil {
          				go func() {
          					rpcs.ServeConn(conn)
          					conn.Close()
          				}()
          			} else {
          				debug("RegistrationServer: accept error", err)
          				break
          			}
          		}
          		debug("RegistrationServer: done\n")
          	}()
          }
          ```

      ​

  - Worker通过`RunWorker`这个函数来启动，每个Worker启动时除了设置自己的一些信息之外，也会注册RPC服务，并且通过`wk.register(MasterAddress)`来远程调用Master的`Register`函数，告诉Master，我已经准备好了（将自己写入到Master结构体中的Workers数组中），然后广播

    - ```go
      // Tell the master we exist and ready to work
      func (wk *Worker) register(master string) {
      	args := new(RegisterArgs)
      	args.Worker = wk.name
      	ok := call(master, "Master.Register", args, new(struct{}))
      	if ok == false {
      		fmt.Printf("Register: RPC %s register error\n", master)
      	}
      }

      // Register is an RPC method that is called by workers after they have started
      // up to report that they are ready to receive tasks.
      func (mr *Master) Register(args *RegisterArgs, _ *struct{}) error {
      	mr.Lock()
      	defer mr.Unlock()
      	debug("Register: worker %s\n", args.Worker)
      	mr.workers = append(mr.workers, args.Worker)

      	// tell forwardRegistrations() that there's a new workers[] entry.
      	mr.newCond.Broadcast()

      	return nil
      }
      ```

  - Master启动之后，会一直等待Worker的连接，阻塞于`mr.newCond.Wait()`直到收到广播通知，这时Master会先确认一下，是否真的有Worker已经准备好了（`len(mr.Workers) > i`)这个条件满足，则将准备好的Worker放入到专门用来执行任务的`channel`

    - ```go
      // helper function that sends information about all existing
      // and newly registered workers to channel ch. schedule()
      // reads ch to learn about workers.
      func (mr *Master) forwardRegistrations(ch chan string) {
      	i := 0
      	for {
      		mr.Lock()
      		if len(mr.workers) > i {
      			// there's a worker that we haven't told schedule() about.
      			w := mr.workers[i]
      			go func() { ch <- w }() // send without holding the lock.
      			i = i + 1
      		} else {
      			// wait for Register() to add an entry to workers[]
      			// in response to an RPC from a new worker.
      			mr.newCond.Wait()
      		}
      		mr.Unlock()
      	}
      }
      ```

  - 当专门用来执行任务的`registerChannel`有Worker到来时，Master取出这个Worker，通过远程过程调用将任务分配给它，等待这个Worker执行完任务之后，再将这个空闲的Worker放入到`registerChannel`中等待下一个任务的分配，由于必须要等待所有的Map任务完成或者Reduce任务完成才算结束，因此使用`sync.WaitGroup`来进行同步

    - ```go
      //
      // schedule() starts and waits for all tasks in the given phase (Map
      // or Reduce). the mapFiles argument holds the names of the files that
      // are the inputs to the map phase, one per map task. nReduce is the
      // number of reduce tasks. the registerChan argument yields a stream
      // of registered workers; each item is the worker's RPC address,
      // suitable for passing to call(). registerChan will yield all
      // existing registered workers (if any) and new ones as they register.
      //
      func schedule(jobName string, mapFiles []string, nReduce int, phase jobPhase, registerChan chan string) {
      	var ntasks int
      	var nOther int // number of inputs (for reduce) or outputs (for map)
      	switch phase {
      	case mapPhase:
      		ntasks = len(mapFiles)
      		nOther = nReduce
      	case reducePhase:
      		ntasks = nReduce
      		nOther = len(mapFiles)
      	}

      	fmt.Printf("Schedule: %v %v tasks (%d I/Os)\n", ntasks, phase, nOther)

      	// All ntasks tasks have to be scheduled on workers, and only once all of
      	// them have been completed successfully should the function return.
      	// Remember that workers may fail, and that any given worker may finish
      	// multiple tasks.
      	//
      	// TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO
      	//
      	var waitGroup sync.WaitGroup
      	for taskID := 0; taskID < ntasks; taskID++ {

      		waitGroup.Add(1)

      		go func(taskID int) {
      			defer waitGroup.Done()

      			var args DoTaskArgs
      			args.JobName = jobName
      			args.Phase = phase
      			args.TaskNumber = taskID
      			args.NumOtherPhase = nOther
      			if args.Phase == mapPhase {
      				args.File = mapFiles[taskID]
      			}

      			for {
      				wkAddr := <-registerChan
      				ok := call(wkAddr, "Worker.DoTask", &args, nil)
      				if ok {
      					go func() {
      						registerChan <- wkAddr
      					}()
      					break
      				}
      			}

      		}(taskID)
      	}

      	waitGroup.Wait()

      	fmt.Printf("Schedule: %v phase done\n", phase)
      }
      ```

    - ```go
      // DoTask is called by the master when a new task is being scheduled on this
      // worker.
      func (wk *Worker) DoTask(arg *DoTaskArgs, _ *struct{}) error {
      	fmt.Printf("%s: given %v task #%d on file %s (nios: %d)\n",
      		wk.name, arg.Phase, arg.TaskNumber, arg.File, arg.NumOtherPhase)

      	wk.Lock()
      	wk.nTasks += 1
      	wk.concurrent += 1
      	nc := wk.concurrent
      	wk.Unlock()

      	if nc > 1 {
      		// schedule() should never issue more than one RPC at a
      		// time to a given worker.
      		log.Fatal("Worker.DoTask: more than one DoTask sent concurrently to a single worker\n")
      	}

      	switch arg.Phase {
      	case mapPhase:
      		doMap(arg.JobName, arg.TaskNumber, arg.File, arg.NumOtherPhase, wk.Map)
      	case reducePhase:
      		doReduce(arg.JobName, arg.TaskNumber, mergeName(arg.JobName, arg.TaskNumber), arg.NumOtherPhase, wk.Reduce)
      	}

      	wk.Lock()
      	wk.concurrent -= 1
      	wk.Unlock()

      	fmt.Printf("%s: %v task #%d done\n", wk.name, arg.Phase, arg.TaskNumber)
      	return nil
      }
      ```

  - 当完成所有任务之后，Master调用`finish`函数，通过远程过程调用Worker的`Shutdown`函数来关闭Worker，并且也关闭自身的RPC服务（位于master.go文件中的Distributed函数）

    - ```go
      func() {
      			mr.stats = mr.killWorkers()
      			mr.stopRPCServer()
      		})
      ```

